{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxeEyIwwiQ3N"
      },
      "source": [
        "# Assignment 2: Advanced RAG Techniques\n",
        "## Day 6 Session 2 - Advanced RAG Fundamentals\n",
        "\n",
        "**OBJECTIVE:** Implement advanced RAG techniques including postprocessors, response synthesizers, and structured outputs.\n",
        "\n",
        "**LEARNING GOALS:**\n",
        "- Understand and implement node postprocessors for filtering and reranking\n",
        "- Learn different response synthesis strategies (TreeSummarize, Refine)\n",
        "- Create structured outputs using Pydantic models\n",
        "- Build advanced retrieval pipelines with multiple processing stages\n",
        "\n",
        "**DATASET:** Use the same data folder as Assignment 1 (`Day_6/session_2/data/`)\n",
        "\n",
        "**PREREQUISITES:** Complete Assignment 1 first\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "1. Complete each function by replacing the TODO comments with actual implementation\n",
        "2. Run each cell after completing the function to test it\n",
        "3. The answers can be found in the `03_advanced_rag_techniques.ipynb` notebook\n",
        "4. Each technique builds on the previous one\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3caZ-ZnjjI0S",
        "outputId": "78225468-df30-4b2b-8a92-db6df9283aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r \"/content/drive/MyDrive/Outskill_C4/requirements.txt\""
      ],
      "metadata": {
        "id": "X33tKB1ija-T",
        "outputId": "1a38eb86-7737-473e-abb8-e70e6904b617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: huggingface-hub 1.3.7 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# securely input your key\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter key\")\n",
        "print(\"âœ“ OpenrRouter key set successfully\")"
      ],
      "metadata": {
        "id": "5L92RaUkjvcE",
        "outputId": "5b393adb-f9e9-4c11-9400-4d218e6ef408",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter keyÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ“ OpenrRouter key set successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_nasE-iQ3O"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for advanced RAG\n",
        "import os\n",
        "import lancedb\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Core LlamaIndex components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "# Vector store\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "\n",
        "# Embeddings and LLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "\n",
        "# Advanced RAG components (we'll use these in the assignments)\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n",
        "from llama_index.core.output_parsers import PydanticOutputParser\n",
        "\n",
        "print(\"âœ… Advanced RAG libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHB9igTZiQ3P"
      },
      "outputs": [],
      "source": [
        "# Configure Advanced RAG Settings (Using OpenRouter)\n",
        "def setup_advanced_rag_settings():\n",
        "    \"\"\"\n",
        "    Configure LlamaIndex with optimized settings for advanced RAG.\n",
        "    Uses local embeddings and OpenRouter for LLM operations.\n",
        "    \"\"\"\n",
        "    # Check for OpenRouter API key\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"âš ï¸  OPENROUTER_API_KEY not found - LLM operations will be limited\")\n",
        "        print(\"   You can still complete postprocessor and retrieval exercises\")\n",
        "    else:\n",
        "        print(\"âœ… OPENROUTER_API_KEY found - full advanced RAG functionality available\")\n",
        "\n",
        "        # Configure OpenRouter LLM\n",
        "        Settings.llm = OpenRouter(\n",
        "            api_key=api_key,\n",
        "            model=\"gpt-4o\",\n",
        "            temperature=0.1  # Lower temperature for more consistent responses\n",
        "        )\n",
        "\n",
        "    # Configure local embeddings (no API key required)\n",
        "    Settings.embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Advanced RAG configuration\n",
        "    Settings.chunk_size = 512  # Smaller chunks for better precision\n",
        "    Settings.chunk_overlap = 50\n",
        "\n",
        "    print(\"âœ… Advanced RAG settings configured\")\n",
        "    print(\"   - Chunk size: 512 (optimized for precision)\")\n",
        "    print(\"   - Using local embeddings for cost efficiency\")\n",
        "    print(\"   - OpenRouter LLM ready for response synthesis\")\n",
        "\n",
        "# Setup the configuration\n",
        "setup_advanced_rag_settings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMX2VfcViQ3P"
      },
      "outputs": [],
      "source": [
        "# Setup: Create index from Assignment 1 (reuse the basic functionality)\n",
        "def setup_basic_index(data_folder: str = \"../data\", force_rebuild: bool = False):\n",
        "    \"\"\"\n",
        "    Create a basic vector index that we'll enhance with advanced techniques.\n",
        "    This reuses the concepts from Assignment 1.\n",
        "    \"\"\"\n",
        "    # Create vector store\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=\"./advanced_rag_vectordb\",\n",
        "        table_name=\"documents\"\n",
        "    )\n",
        "\n",
        "    # Load documents\n",
        "    if not Path(data_folder).exists():\n",
        "        print(f\"âŒ Data folder not found: {data_folder}\")\n",
        "        return None\n",
        "\n",
        "    reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
        "    documents = reader.load_data()\n",
        "\n",
        "    # Create storage context and index\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Basic index created with {len(documents)} documents\")\n",
        "    print(\"   Ready for advanced RAG techniques!\")\n",
        "    return index\n",
        "\n",
        "# Create the basic index\n",
        "print(\"ğŸ“ Setting up basic index for advanced RAG...\")\n",
        "index = setup_basic_index()\n",
        "\n",
        "if index:\n",
        "    print(\"ğŸš€ Ready to implement advanced RAG techniques!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to create index - check data folder path\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwKxjpg3iQ3P"
      },
      "source": [
        "## 1. Node Postprocessors - Similarity Filtering\n",
        "\n",
        "**Concept:** Postprocessors refine retrieval results after the initial vector search. The `SimilarityPostprocessor` filters out chunks that fall below a relevance threshold.\n",
        "\n",
        "**Why it matters:** Raw vector search often returns some irrelevant results. Filtering improves precision and response quality.\n",
        "\n",
        "Complete the function below to create a query engine with similarity filtering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dud6xD-riQ3P"
      },
      "outputs": [],
      "source": [
        "def create_query_engine_with_similarity_filter(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n",
        "    \"\"\"\n",
        "    Create a query engine that filters results based on similarity scores.\n",
        "\n",
        "    TODO: Complete this function to create a query engine with similarity postprocessing.\n",
        "    HINT: Use index.as_query_engine() with node_postprocessors parameter containing SimilarityPostprocessor\n",
        "\n",
        "    Args:\n",
        "        index: Vector index to query\n",
        "        similarity_cutoff: Minimum similarity score (0.0 to 1.0)\n",
        "        top_k: Number of initial results to retrieve before filtering\n",
        "\n",
        "    Returns:\n",
        "        Query engine with similarity filtering\n",
        "    \"\"\"\n",
        "    # TODO: Create similarity postprocessor with the cutoff threshold\n",
        "    # similarity_processor = ?\n",
        "\n",
        "    # TODO: Create query engine with similarity filtering\n",
        "    # query_engine = ?\n",
        "\n",
        "    # return query_engine\n",
        "\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Create query engine with similarity cutoff {similarity_cutoff}\")\n",
        "    return None\n",
        "\n",
        "# Test the function\n",
        "if index:\n",
        "    filtered_engine = create_query_engine_with_similarity_filter(index, similarity_cutoff=0.3)\n",
        "\n",
        "    if filtered_engine:\n",
        "        print(\"âœ… Query engine with similarity filtering created\")\n",
        "\n",
        "        # Test query\n",
        "        test_query = \"What are the benefits of AI agents?\"\n",
        "        print(f\"\\nğŸ” Testing query: '{test_query}'\")\n",
        "\n",
        "        # Uncomment when implemented:\n",
        "        # response = filtered_engine.query(test_query)\n",
        "        # print(f\"ğŸ“ Response: {response}\")\n",
        "        print(\"   (Complete the function above to test the response)\")\n",
        "    else:\n",
        "        print(\"âŒ Failed to create filtered query engine\")\n",
        "else:\n",
        "    print(\"âŒ No index available - run previous cells first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJfgaGjNiQ3Q"
      },
      "source": [
        "## 2. Response Synthesizers - TreeSummarize\n",
        "\n",
        "**Concept:** Response synthesizers control how retrieved information becomes final answers. `TreeSummarize` builds responses hierarchically, ideal for complex analytical questions.\n",
        "\n",
        "**Why it matters:** Different synthesis strategies work better for different query types. TreeSummarize excels at comprehensive analysis and long-form responses.\n",
        "\n",
        "Complete the function below to create a query engine with TreeSummarize response synthesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MbPkhk5iQ3Q"
      },
      "outputs": [],
      "source": [
        "def create_query_engine_with_tree_summarize(index, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Create a query engine that uses TreeSummarize for comprehensive responses.\n",
        "\n",
        "    TODO: Complete this function to create a query engine with TreeSummarize synthesis.\n",
        "    HINT: Create a TreeSummarize instance, then use index.as_query_engine() with response_synthesizer parameter\n",
        "\n",
        "    Args:\n",
        "        index: Vector index to query\n",
        "        top_k: Number of results to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Query engine with TreeSummarize synthesis\n",
        "    \"\"\"\n",
        "    # TODO: Create TreeSummarize response synthesizer\n",
        "    # tree_synthesizer =\n",
        "\n",
        "    # TODO: Create query engine with the synthesizer\n",
        "    # query_engine =\n",
        "\n",
        "    # return query_engine\n",
        "\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Create query engine with TreeSummarize synthesis\")\n",
        "    return None\n",
        "\n",
        "# Test the function\n",
        "if index:\n",
        "    tree_engine = create_query_engine_with_tree_summarize(index)\n",
        "\n",
        "    if tree_engine:\n",
        "        print(\"âœ… Query engine with TreeSummarize created\")\n",
        "\n",
        "        # Test with a complex analytical query\n",
        "        analytical_query = \"Compare the advantages and disadvantages of different AI agent frameworks\"\n",
        "        print(f\"\\nğŸ” Testing analytical query: '{analytical_query}'\")\n",
        "\n",
        "        # Uncomment when implemented:\n",
        "        # response = tree_engine.query(analytical_query)\n",
        "        # print(f\"ğŸ“ TreeSummarize Response:\\n{response}\")\n",
        "        print(\"   (Complete the function above to test comprehensive analysis)\")\n",
        "    else:\n",
        "        print(\"âŒ Failed to create TreeSummarize query engine\")\n",
        "else:\n",
        "    print(\"âŒ No index available - run previous cells first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT_D2JqDiQ3Q"
      },
      "source": [
        "## 3. Structured Outputs with Pydantic Models\n",
        "\n",
        "**Concept:** Structured outputs ensure predictable, parseable responses using Pydantic models. This is essential for API endpoints and data pipelines.\n",
        "\n",
        "**Why it matters:** Instead of free-text responses, you get type-safe, validated data structures that applications can reliably process.\n",
        "\n",
        "Complete the function below to create a structured output system for extracting research paper information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GK0wi8biQ3Q"
      },
      "outputs": [],
      "source": [
        "# First, define the Pydantic models for structured outputs\n",
        "class ResearchPaperInfo(BaseModel):\n",
        "    \"\"\"Structured information about a research paper or AI concept.\"\"\"\n",
        "    title: str = Field(description=\"The main title or concept name\")\n",
        "    key_points: List[str] = Field(description=\"3-5 main points or findings\")\n",
        "    applications: List[str] = Field(description=\"Practical applications or use cases\")\n",
        "    summary: str = Field(description=\"Brief 2-3 sentence summary\")\n",
        "\n",
        "# Import the missing component\n",
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "\n",
        "def create_structured_output_program(output_model: BaseModel = ResearchPaperInfo):\n",
        "    \"\"\"\n",
        "    Create a structured output program using Pydantic models.\n",
        "\n",
        "    TODO: Complete this function to create a structured output program.\n",
        "    HINT: Use LLMTextCompletionProgram.from_defaults() with PydanticOutputParser and a prompt template\n",
        "\n",
        "    Args:\n",
        "        output_model: Pydantic model class for structured output\n",
        "\n",
        "    Returns:\n",
        "        LLMTextCompletionProgram that returns structured data\n",
        "    \"\"\"\n",
        "    # TODO: Create output parser with the Pydantic model\n",
        "    # output_parser = ?\n",
        "\n",
        "    # TODO: Create the structured output program\n",
        "    # program = ?\n",
        "\n",
        "    # return program\n",
        "\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Create structured output program with {output_model.__name__}\")\n",
        "    return None\n",
        "\n",
        "# Test the function\n",
        "if index:\n",
        "    structured_program = create_structured_output_program(ResearchPaperInfo)\n",
        "\n",
        "    if structured_program:\n",
        "        print(\"âœ… Structured output program created\")\n",
        "\n",
        "        # Test with retrieval and structured extraction\n",
        "        structure_query = \"Tell me about AI agents and their capabilities\"\n",
        "        print(f\"\\nğŸ” Testing structured query: '{structure_query}'\")\n",
        "\n",
        "        # Get context for structured extraction (Uncomment when implemented)\n",
        "        # retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n",
        "        # nodes = retriever.retrieve(structure_query)\n",
        "        # context = \"\\n\".join([node.text for node in nodes])\n",
        "\n",
        "        # Uncomment when implemented:\n",
        "        # response = structured_program(context=context, query=structure_query)\n",
        "        # print(f\"ğŸ“Š Structured Response:\\n{response}\")\n",
        "        print(\"   (Complete the function above to get structured JSON output)\")\n",
        "\n",
        "        print(\"\\nğŸ’¡ Expected output format:\")\n",
        "        print(\"   - title: String\")\n",
        "        print(\"   - key_points: List of strings\")\n",
        "        print(\"   - applications: List of strings\")\n",
        "        print(\"   - summary: String\")\n",
        "    else:\n",
        "        print(\"âŒ Failed to create structured output program\")\n",
        "else:\n",
        "    print(\"âŒ No index available - run previous cells first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCDHaWqyiQ3R"
      },
      "source": [
        "## 4. Advanced Pipeline - Combining All Techniques\n",
        "\n",
        "**Concept:** Combine multiple advanced techniques into a single powerful query engine: similarity filtering + response synthesis + structured output.\n",
        "\n",
        "**Why it matters:** Production RAG systems often need multiple techniques working together for optimal results.\n",
        "\n",
        "Complete the function below to create a comprehensive advanced RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p05mwdqiQ3R"
      },
      "outputs": [],
      "source": [
        "def create_advanced_rag_pipeline(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n",
        "    \"\"\"\n",
        "    Create a comprehensive advanced RAG pipeline combining multiple techniques.\n",
        "\n",
        "    TODO: Complete this function to create the ultimate advanced RAG query engine.\n",
        "    HINT: Combine SimilarityPostprocessor + TreeSummarize using index.as_query_engine()\n",
        "\n",
        "    Args:\n",
        "        index: Vector index to query\n",
        "        similarity_cutoff: Minimum similarity score for filtering\n",
        "        top_k: Number of initial results to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Advanced query engine with filtering and synthesis combined\n",
        "    \"\"\"\n",
        "    # TODO: Create similarity postprocessor\n",
        "    # similarity_processor = ?\n",
        "\n",
        "    # TODO: Create TreeSummarize for comprehensive responses\n",
        "    # tree_synthesizer = ?\n",
        "\n",
        "    # TODO: Create the comprehensive query engine combining both techniques\n",
        "    # advanced_engine = ?\n",
        "\n",
        "    # return advanced_engine\n",
        "\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Create advanced RAG pipeline with all techniques\")\n",
        "    return None\n",
        "\n",
        "# Test the comprehensive pipeline\n",
        "if index:\n",
        "    advanced_pipeline = create_advanced_rag_pipeline(index)\n",
        "\n",
        "    if advanced_pipeline:\n",
        "        print(\"âœ… Advanced RAG pipeline created successfully!\")\n",
        "        print(\"   ğŸ”§ Similarity filtering: âœ…\")\n",
        "        print(\"   ğŸŒ³ TreeSummarize synthesis: âœ…\")\n",
        "\n",
        "        # Test with complex query\n",
        "        complex_query = \"Analyze the current state and future potential of AI agent technologies\"\n",
        "        print(f\"\\nğŸ” Testing complex query: '{complex_query}'\")\n",
        "\n",
        "        # Uncomment when implemented:\n",
        "        # response = advanced_pipeline.query(complex_query)\n",
        "        # print(f\"ğŸš€ Advanced RAG Response:\\n{response}\")\n",
        "        print(\"   (Complete the function above to test the full pipeline)\")\n",
        "\n",
        "        print(\"\\nğŸ¯ This should provide:\")\n",
        "        print(\"   - Filtered relevant results only\")\n",
        "        print(\"   - Comprehensive analytical response\")\n",
        "        print(\"   - Combined postprocessing and synthesis\")\n",
        "    else:\n",
        "        print(\"âŒ Failed to create advanced RAG pipeline\")\n",
        "else:\n",
        "    print(\"âŒ No index available - run previous cells first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Mgy9AriQ3R"
      },
      "source": [
        "## 5. Final Test - Compare Basic vs Advanced RAG\n",
        "\n",
        "Once you've completed all the functions above, run this cell to compare basic RAG with your advanced techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoVYPyYqiQ3R"
      },
      "outputs": [],
      "source": [
        "# Final comparison: Basic vs Advanced RAG\n",
        "print(\"ğŸš€ Advanced RAG Techniques Assignment - Final Test\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test queries for comparison\n",
        "test_queries = [\n",
        "    \"What are the key capabilities of AI agents?\",\n",
        "    \"How do you evaluate agent performance metrics?\",\n",
        "    \"Explain the benefits and challenges of multimodal AI systems\"\n",
        "]\n",
        "\n",
        "# Check if all components were created\n",
        "components_status = {\n",
        "    \"Basic Index\": index is not None,\n",
        "    \"Similarity Filter\": 'filtered_engine' in locals() and filtered_engine is not None,\n",
        "    \"TreeSummarize\": 'tree_engine' in locals() and tree_engine is not None,\n",
        "    \"Structured Output\": 'structured_program' in locals() and structured_program is not None,\n",
        "    \"Advanced Pipeline\": 'advanced_pipeline' in locals() and advanced_pipeline is not None\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“Š Component Status:\")\n",
        "for component, status in components_status.items():\n",
        "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
        "    print(f\"   {status_icon} {component}\")\n",
        "\n",
        "# Create basic query engine for comparison\n",
        "if index:\n",
        "    print(\"\\nğŸ” Creating basic query engine for comparison...\")\n",
        "    basic_engine = index.as_query_engine(similarity_top_k=5)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ†š COMPARISON: Basic vs Advanced RAG\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\nğŸ“‹ Test Query {i}: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Basic RAG\n",
        "        print(\"ğŸ”¹ Basic RAG:\")\n",
        "        if basic_engine:\n",
        "            # Uncomment when testing:\n",
        "            # basic_response = basic_engine.query(query)\n",
        "            # print(f\"   Response: {str(basic_response)[:200]}...\")\n",
        "            print(\"   (Standard vector search + simple response)\")\n",
        "\n",
        "        # Advanced RAG (if implemented)\n",
        "        print(\"\\nğŸ”¸ Advanced RAG:\")\n",
        "        if components_status[\"Advanced Pipeline\"]:\n",
        "            # Uncomment when testing:\n",
        "            # advanced_response = advanced_pipeline.query(query)\n",
        "            # print(f\"   Response: {advanced_response}\")\n",
        "            print(\"   (Filtered + TreeSummarize + Structured output)\")\n",
        "        else:\n",
        "            print(\"   Complete the advanced pipeline function to test\")\n",
        "\n",
        "# Final status\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ¯ Assignment Status:\")\n",
        "completed_count = sum(components_status.values())\n",
        "total_count = len(components_status)\n",
        "\n",
        "print(f\"   Completed: {completed_count}/{total_count} components\")\n",
        "\n",
        "if completed_count == total_count:\n",
        "    print(\"\\nğŸ‰ Congratulations! You've mastered Advanced RAG Techniques!\")\n",
        "    print(\"   âœ… Node postprocessors for result filtering\")\n",
        "    print(\"   âœ… Response synthesizers for better answers\")\n",
        "    print(\"   âœ… Structured outputs for reliable data\")\n",
        "    print(\"   âœ… Advanced pipelines combining all techniques\")\n",
        "    print(\"\\nğŸš€ You're ready for production RAG systems!\")\n",
        "else:\n",
        "    missing = total_count - completed_count\n",
        "    print(f\"\\nğŸ“ Complete {missing} more components to finish the assignment:\")\n",
        "    for component, status in components_status.items():\n",
        "        if not status:\n",
        "            print(f\"   - {component}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Key learnings:\")\n",
        "print(\"   - Postprocessors improve result relevance and precision\")\n",
        "print(\"   - Different synthesizers work better for different query types\")\n",
        "print(\"   - Structured outputs enable reliable system integration\")\n",
        "print(\"   - Advanced techniques can be combined for production systems\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}